# ğŸ° Multi-Armed Bandit Problem using Îµ-Greedy and UCB

##  Problem Statement

This assignment focuses on solving the **Multi-Armed Bandit (MAB)** problem using two action selection strategies: **Îµ-greedy** and **Upper Confidence Bound (UCB)**.  
The objective is to maximize the total reward ğŸ¯ by selecting the best possible action while maintaining a balance between **exploration** ğŸ” and **exploitation** ğŸ’¡.

---

##  Algorithms Implemented

### Îµ-Greedy Algorithm
In the Îµ-greedy method, the agent selects a random action with probability Îµ (exploration ğŸ”„) and selects the action with the highest estimated reward with probability (1 âˆ’ Îµ) (exploitation â­).  
The value of Îµ is varied from **0.01 to 0.99** with a step size of **0.1** to analyze its effect on performance.

### Upper Confidence Bound (UCB) Algorithm
The UCB algorithm selects actions based on both estimated reward and uncertainty. The action selection formula used is:

\[
UCB(a) = Q(a) + \sqrt{\frac{2 \ln n}{n_a}}
\]

where \(Q(a)\) is the estimated action value and the second term represents the uncertainty ğŸ“Š.

---

## ğŸ” Reward Estimation

The estimated reward for each action is updated using **incremental sample averaging**, which allows efficient learning without storing all past rewards.

---

## âš™ï¸ Experimental Setup

- Number of arms: 5  
- Reward distribution: Bernoulli ğŸ²  
- Total iterations: 10,000  
- Each arm is selected at least once during initialization  

---

## ğŸ“ˆ Performance Evaluation

The performance of the algorithms is evaluated using:
- Average reward over time ğŸ“‰  
- Percentage of optimal action selection âœ…  

---

## ğŸ“Š Output and Analysis

- A plot of **Average Reward at 10,000 steps vs Îµ**  
- Learning curve comparison of **Îµ-greedy** and **UCB** based on average reward and optimal action percentage  

---

## ğŸ Conclusion

The results show how exploration strategies affect learning efficiency. UCB generally performs better in early stages due to systematic exploration, while Îµ-greedy performance depends on the choice of Îµ.
